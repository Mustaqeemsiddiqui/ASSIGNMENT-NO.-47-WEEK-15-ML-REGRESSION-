{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6141529-7d6c-4efa-90d5-4aefb30519b8",
   "metadata": {},
   "source": [
    "**Q1. What is Lasso Regression, and how does it differ from other regression techniques?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4490c3de-751d-4584-892a-0c095bb61d3f",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "Lasso Regression, which stands for Least Absolute Shrinkage and Selection Operator, is a type of linear regression that incorporates regularization. It aims to enhance prediction accuracy and interpretability by modifying the loss function to include a penalty term that enforces sparsity. Here’s an overview of Lasso Regression and how it differs from other regression techniques:\n",
    "\n",
    "### Key Concepts of Lasso Regression\n",
    "\n",
    "1. **Loss Function with L1 Regularization**:\n",
    "   - The loss function for Lasso Regression includes an L1 regularization term, which is the sum of the absolute values of the coefficients.\n",
    "   - The Lasso loss function is given by:\n",
    "     \\[\n",
    "     \\text{RSS}_{\\text{lasso}} = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i \\beta)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "     \\]\n",
    "     where \\( y_i \\) are the observed values, \\( \\mathbf{x}_i \\) are the predictor variables, \\( \\beta \\) are the coefficients, and \\( \\lambda \\) is the regularization parameter.\n",
    "\n",
    "2. **Coefficient Shrinkage and Selection**:\n",
    "   - The L1 penalty term causes some coefficients to be exactly zero when the regularization parameter \\(\\lambda\\) is sufficiently large.\n",
    "   - This property makes Lasso Regression useful for feature selection, as it can shrink less important feature coefficients to zero, effectively excluding them from the model.\n",
    "\n",
    "### Differences from Other Regression Techniques\n",
    "\n",
    "1. **Ordinary Least Squares (OLS) Regression**:\n",
    "   - OLS minimizes the sum of squared residuals without any penalty term.\n",
    "   - OLS can suffer from overfitting, especially when there are many predictors or multicollinearity among predictors.\n",
    "\n",
    "2. **Ridge Regression**:\n",
    "   - Ridge Regression includes an L2 regularization term, which is the sum of the squares of the coefficients.\n",
    "   - The Ridge loss function is given by:\n",
    "     \\[\n",
    "     \\text{RSS}_{\\text{ridge}} = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i \\beta)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "     \\]\n",
    "   - Ridge Regression shrinks coefficients but does not set any coefficients to zero, so it does not perform feature selection.\n",
    "\n",
    "3. **Elastic Net Regression**:\n",
    "   - Elastic Net combines both L1 and L2 regularization.\n",
    "   - The Elastic Net loss function is given by:\n",
    "     \\[\n",
    "     \\text{RSS}_{\\text{elastic}} = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i \\beta)^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\n",
    "     \\]\n",
    "   - Elastic Net can perform both coefficient shrinkage and feature selection while handling multicollinearity effectively.\n",
    "\n",
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Generate Example Data**:\n",
    "   - Create a synthetic dataset with 100 samples and 10 features. The target variable \\(y\\) is a linear combination of the first two features plus some noise.\n",
    "\n",
    "2. **Train-Test Split**:\n",
    "   - Split the data into training and test sets.\n",
    "\n",
    "3. **Standardization**:\n",
    "   - Standardize the features to ensure that regularization affects all predictors equally.\n",
    "\n",
    "4. **Fit Lasso Regression**:\n",
    "   - Fit a Lasso Regression model on the training data with a regularization parameter \\(\\alpha\\).\n",
    "\n",
    "5. **Predict and Evaluate**:\n",
    "   - Make predictions on the test set and calculate the Mean Squared Error (MSE) to evaluate the model's performance.\n",
    "\n",
    "6. **Print Coefficients**:\n",
    "   - Extract and print the coefficients to see which features have been selected by the Lasso model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f8d35b-58b1-44aa-8d8e-8ea764116796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression MSE: 0.8449\n",
      "Lasso Coefficients: [ 0.71994852  0.39039068  0.          0.         -0.01466975  0.\n",
      " -0.13471527  0.          0.         -0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 100, 10\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = X[:, 0] + 0.5 * X[:, 1] + np.random.randn(n_samples)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Lasso Regression MSE: {mse:.4f}')\n",
    "\n",
    "# Print coefficients\n",
    "print('Lasso Coefficients:', lasso.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee766a-d9b1-40e2-b6ee-aea08dab5eb0",
   "metadata": {},
   "source": [
    "**Q2. What is the main advantage of using Lasso Regression in feature selection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b3ffc-cbd9-46cf-be5b-e1960f4cda09",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "The main advantage of using Lasso Regression for feature selection lies in its ability to automatically select a subset of relevant features while shrinking the coefficients of less important features to zero. This property is particularly advantageous in the following ways:\n",
    "\n",
    "1. **Automatic Feature Selection**:\n",
    "   - Lasso Regression includes an L1 regularization term in its objective function, which penalizes the sum of the absolute values of the coefficients (\\(\\sum_{j=1}^{p} |\\beta_j|\\)).\n",
    "   - As a result, many coefficients can be exactly zero when the regularization parameter \\(\\lambda\\) is sufficiently large.\n",
    "   - This automatic selection of features effectively performs feature selection during model fitting, reducing the number of predictors and potentially improving model interpretability.\n",
    "\n",
    "2. **Reduction of Overfitting**:\n",
    "   - By setting coefficients of less relevant features to zero, Lasso Regression reduces the complexity of the model.\n",
    "   - This reduction in complexity helps mitigate overfitting, especially in high-dimensional datasets where the number of predictors (\\(p\\)) is large compared to the number of observations (\\(n\\)).\n",
    "\n",
    "3. **Interpretability**:\n",
    "   - Sparse models resulting from Lasso Regression are easier to interpret because they include only the most relevant features.\n",
    "   - The non-zero coefficients directly indicate the importance and direction (positive or negative impact) of each selected feature on the target variable.\n",
    "\n",
    "4. **Handling Multicollinearity**:\n",
    "   - Lasso Regression can handle multicollinearity (high correlation between predictors) by selecting one from a group of correlated features and setting others to zero.\n",
    "   - This can improve model stability and performance compared to methods like OLS regression, which can struggle with multicollinearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5d7f3-cbfc-48d6-ac58-a1c85f621d68",
   "metadata": {},
   "source": [
    "**Q3. How do you interpret the coefficients of a Lasso Regression model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ebeb2-2011-4dab-a14f-a30f62f12af6",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "In Lasso Regression, interpreting the coefficients involves understanding how each feature contributes to the predicted outcome, while considering the penalty imposed by the regularization. Here’s a general approach to interpreting the coefficients:\n",
    "\n",
    "1. **Magnitude of Coefficients**: \n",
    "   - The coefficients themselves indicate the strength and direction of the relationship between each feature and the target variable. A higher absolute value suggests a stronger impact on the prediction.\n",
    "\n",
    "2. **Sign of Coefficients**: \n",
    "   - The sign (positive or negative) indicates the direction of the relationship:\n",
    "     - Positive coefficient: As the feature increases, the predicted outcome tends to increase.\n",
    "     - Negative coefficient: As the feature increases, the predicted outcome tends to decrease.\n",
    "\n",
    "3. **Regularization Impact**: \n",
    "   - Lasso Regression imposes a penalty on the size of coefficients to prevent overfitting. This can lead to some coefficients being shrunk to zero, effectively excluding those features from the model. \n",
    "   - Features with non-zero coefficients are considered important predictors, while those with coefficients close to zero or zero may have less influence on the model’s predictions.\n",
    "\n",
    "4. **Relative Importance**: \n",
    "   - Comparing the magnitudes of coefficients can give a sense of the relative importance of different features. Larger coefficients generally indicate stronger predictive power, though the exact scale can depend on the scaling of your features.\n",
    "\n",
    "5. **Interaction and Context**: \n",
    "   - Interpretation should consider interactions between features and the context of the problem. A coefficient’s meaning can change depending on other features in the model and the domain knowledge.\n",
    "\n",
    "6. **Bias Term**: \n",
    "   - The intercept (bias term) in Lasso Regression represents the predicted outcome when all features are zero. Its interpretation depends on the scaling and nature of your features.\n",
    "\n",
    "Interpreting Lasso Regression coefficients requires balancing statistical significance with practical significance in the context of your specific dataset and problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed4ed8-bbd1-4f1f-a5ff-443526b60e89",
   "metadata": {},
   "source": [
    "**Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e519d1a-3ab3-4c5b-8ab2-a745383350a7",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "In Lasso Regression, the tuning parameter that can be adjusted is usually denoted as \\(\\alpha\\). This parameter controls the strength of regularization applied to the model. Here’s how it works and its impact on the model's performance:\n",
    "\n",
    "1. **Alpha (\\(\\alpha\\)) Parameter**:\n",
    "   - **Role**: Alpha determines the balance between fitting the data well and keeping the model simple (regularized). It controls the amount of shrinkage applied to the coefficients.\n",
    "   - **Effect**: \n",
    "     - When \\(\\alpha\\) is 0, Lasso Regression behaves like ordinary least squares regression, where there is no penalty for the size of coefficients.\n",
    "     - As \\(\\alpha\\) increases, more coefficients are pushed towards zero, leading to sparsity in the model (some coefficients becoming exactly zero), which simplifies the model and helps prevent overfitting.\n",
    "   \n",
    "2. **Impact on Model Performance**:\n",
    "   - **Underfitting vs. Overfitting**: \n",
    "     - A very high \\(\\alpha\\) can lead to underfitting, where the model is too constrained and fails to capture the underlying patterns in the data.\n",
    "     - A very low \\(\\alpha\\) may result in overfitting, where the model fits the noise in the training data too closely and fails to generalize to new data.\n",
    "   \n",
    "   - **Bias-Variance Tradeoff**:\n",
    "     - Increasing \\(\\alpha\\) increases bias (since it imposes more regularization), but reduces variance by simplifying the model and making it less sensitive to noise in the training data.\n",
    "     - Decreasing \\(\\alpha\\) reduces bias but increases variance, potentially leading to overfitting.\n",
    "\n",
    "3. **Choosing the Right \\(\\alpha\\)**:\n",
    "   - **Cross-Validation**: Typically, \\(\\alpha\\) is chosen using techniques like cross-validation, where different values are tested to find the one that optimizes model performance on unseen data.\n",
    "   - **Grid Search**: Grid search or other optimization techniques can be used to systematically explore different values of \\(\\alpha\\) and find the best performing one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96d863-2a4b-4f80-9555-b20ec5de074d",
   "metadata": {},
   "source": [
    "**Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dfc26-f139-49e5-8416-4cbeacd40671",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "Lasso Regression, as originally formulated, is a linear regression method with L1 regularization. This means it assumes a linear relationship between the features and the target variable. However, it can be extended to handle non-linear regression problems by incorporating non-linear transformations of the original features. Here are a few approaches to adapt Lasso Regression for non-linear regression problems:\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Introduce non-linear transformations of the original features, such as quadratic terms (\\(x_i^2\\)), interaction terms (\\(x_i \\cdot x_j\\)), or other higher-order polynomial terms. For example, if \\(x_i\\) is a feature, adding \\(x_i^2\\), \\(x_i^3\\), etc., allows the model to capture non-linear relationships.\n",
    "\n",
    "2. **Kernel Methods**:\n",
    "   - Use kernel functions to implicitly map the original feature space into a higher-dimensional space where the relationship between features and target variable may become more linear. Common kernels include polynomial kernels and radial basis function (RBF) kernels.\n",
    "\n",
    "3. **Regularization with Non-linear Models**:\n",
    "   - Apply Lasso regularization within a non-linear model framework, such as using Lasso within a Support Vector Machine (SVM) or a neural network. This approach combines the non-linear modeling capacity of these methods with the regularization benefits of Lasso.\n",
    "\n",
    "4. **Penalized Regression with Basis Functions**:\n",
    "   - Use basis functions (e.g., spline basis functions) to transform the original features into a space where the relationship between features and target variable is approximately linear. Then, apply Lasso regularization in this transformed space.\n",
    "\n",
    "5. **Regularization with Decision Trees**:\n",
    "   - Regularize decision trees using Lasso to prevent overfitting and encourage sparsity in the tree structure, thereby promoting simpler models.\n",
    "\n",
    "6. **Ensemble Methods**:\n",
    "   - Combine multiple Lasso Regression models trained on different subsets of data or with different regularization strengths to capture complex, non-linear relationships collectively.\n",
    "\n",
    "In practice, the choice of method depends on the specific characteristics of the non-linearities in the data and the desired interpretability of the model. While Lasso Regression itself assumes linearity, these adaptations allow it to be effective in addressing non-linear regression tasks by leveraging transformations and regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beccf41-7758-435c-8c83-f3b043c04b15",
   "metadata": {},
   "source": [
    "**Q6. What is the difference between Ridge Regression and Lasso Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300244a0-c6da-4415-a612-174db9f4056f",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "Ridge Regression and Lasso Regression are both linear regression techniques that introduce regularization to handle multicollinearity and prevent overfitting. Here are the key differences between the two:\n",
    "\n",
    "1. **Type of Regularization**:\n",
    "   - **Ridge Regression**: Uses L2 regularization, which adds a penalty term proportional to the square of the coefficients (\\(\\sum_{j=1}^{p} \\beta_j^2\\)) to the loss function. This encourages smaller coefficients but does not set them exactly to zero.\n",
    "   - **Lasso Regression**: Uses L1 regularization, which adds a penalty term proportional to the absolute value of the coefficients (\\(\\sum_{j=1}^{p} |\\beta_j|\\)) to the loss function. Lasso can lead to some coefficients being exactly zero, effectively performing feature selection.\n",
    "\n",
    "2. **Shrinkage Properties**:\n",
    "   - **Ridge Regression**: Shrinks the coefficients towards zero, but rarely to exactly zero, allowing all features to potentially contribute to the model.\n",
    "   - **Lasso Regression**: Can shrink some coefficients to exactly zero, effectively performing feature selection by eliminating less important predictors from the model.\n",
    "\n",
    "3. **Behavior with Large Coefficients**:\n",
    "   - **Ridge Regression**: Handles multicollinearity well by shrinking large coefficients, reducing the impact of correlated features.\n",
    "   - **Lasso Regression**: Also handles multicollinearity but tends to favor sparse models by selecting only a subset of the most important predictors and setting the coefficients of less important predictors to zero.\n",
    "\n",
    "4. **Computational Considerations**:\n",
    "   - **Ridge Regression**: Typically easier to compute than Lasso because the L2 penalty (squared term) leads to a smooth, convex optimization problem that can be solved efficiently.\n",
    "   - **Lasso Regression**: More computationally intensive than Ridge Regression because the L1 penalty (absolute value term) leads to a non-smooth, convex optimization problem that may require more advanced optimization techniques.\n",
    "\n",
    "5. **Application**:\n",
    "   - **Ridge Regression**: Useful when all features are potentially relevant and you want to avoid overfitting by shrinking the coefficients.\n",
    "   - **Lasso Regression**: Useful when there are many features and you suspect that only a subset of them are relevant, or when you want a simpler, more interpretable model with feature selection capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358fb50c-e9fa-48b8-8955-c483b68fa131",
   "metadata": {},
   "source": [
    "**Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e122903-bcc9-4376-8604-ad4f39d8d698",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity to some extent, but not as effectively as Ridge Regression. Here’s how Lasso Regression addresses multicollinearity:\n",
    "\n",
    "1. **Feature Selection**:\n",
    "   - Lasso Regression performs feature selection by shrinking the coefficients of less important predictors towards zero. When there are highly correlated features (multicollinearity), Lasso tends to select one feature from the group of correlated features and shrink the coefficients of the others to zero. This effectively chooses one representative feature while disregarding the redundant ones.\n",
    "\n",
    "2. **Sparse Solutions**:\n",
    "   - Due to its L1 regularization penalty, Lasso encourages sparsity in the coefficient vector. When faced with multicollinearity, Lasso may set the coefficients of correlated features to zero, effectively choosing the most relevant feature or a combination that best represents the group.\n",
    "\n",
    "3. **Limitations**:\n",
    "   - Lasso Regression is not as robust to multicollinearity as Ridge Regression because the L1 penalty tends to select features in a more arbitrary manner when they are highly correlated. This can lead to instability in the selected features depending on small changes in the dataset or the regularization parameter.\n",
    "\n",
    "4. **Comparison with Ridge Regression**:\n",
    "   - Ridge Regression (which uses L2 regularization) is generally preferred for handling multicollinearity because it shrinks the coefficients of correlated features towards each other, without setting them exactly to zero. This helps to maintain stability and reduces the impact of multicollinearity on the model’s performance.\n",
    "\n",
    "In practice, when dealing with multicollinearity, the choice between Lasso and Ridge Regression depends on whether feature selection (Lasso’s ability to set coefficients to zero) or multicollinearity handling (Ridge’s ability to shrink coefficients without eliminating them) is more important for the specific modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1953cd6-bf2c-4308-aef7-76991aed8c53",
   "metadata": {},
   "source": [
    "**Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38722d3e-fb2b-4533-a2ee-c5e007f49566",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "Choosing the optimal value of the regularization parameter (often denoted as \\(\\lambda\\) or \\(\\alpha\\)) in Lasso Regression is crucial for obtaining a well-performing model that balances bias and variance effectively. Here’s a step-by-step approach to determine the optimal value:\n",
    "\n",
    "1. **Cross-Validation**:\n",
    "   - **K-Fold Cross-Validation**: Split your data into \\(k\\) folds. For each candidate value of \\(\\lambda\\):\n",
    "     - Train the Lasso Regression model on \\(k-1\\) folds.\n",
    "     - Validate the model on the remaining fold (validation fold).\n",
    "     - Repeat this process for each fold to obtain an average validation error (such as mean squared error).\n",
    "   - **Grid Search**: Evaluate the model’s performance across a range of \\(\\lambda\\) values (grid search) to identify the value that minimizes the average validation error. Grid search involves selecting a range of \\(\\lambda\\) values and testing them systematically.\n",
    "\n",
    "2. **Regularization Path**:\n",
    "   - **Plotting Coefficients**: Plot the coefficients of the Lasso model against different values of \\(\\lambda\\). This can help visualize how the coefficients change with varying regularization strength. The optimal \\(\\lambda\\) value typically occurs where the model achieves a balance between bias and variance, and where the coefficients stabilize or are set to zero.\n",
    "\n",
    "3. **Information Criteria**:\n",
    "   - **AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion)**: These criteria penalize model complexity (number of features) along with the goodness of fit. Lower values indicate a better balance between model fit and complexity.\n",
    "\n",
    "4. **Nested Cross-Validation**:\n",
    "   - For more robust validation, especially when the dataset is small, use nested cross-validation. This involves an outer cross-validation loop to estimate model performance and an inner cross-validation loop to select the best \\(\\lambda\\) value.\n",
    "\n",
    "5. **Regularization Parameter Sensitivity Analysis**:\n",
    "   - Assess the sensitivity of the model’s performance to different \\(\\lambda\\) values. Ensure that the chosen \\(\\lambda\\) value generalizes well to unseen data by validating against a separate test set or using nested cross-validation.\n",
    "\n",
    "6. **Domain Knowledge**:\n",
    "   - Incorporate domain knowledge or prior expectations about the importance of features. Sometimes, specific \\(\\lambda\\) values may align better with what is known about the data and problem context.\n",
    "\n",
    "By systematically evaluating the model’s performance across different \\(\\lambda\\) values using cross-validation or information criteria, you can select the optimal regularization parameter that minimizes overfitting while maintaining model interpretability and predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c246f-334a-41bf-9acc-d2ba6be767bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
